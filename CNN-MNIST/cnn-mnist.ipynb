{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-04T10:22:09.850523Z","iopub.execute_input":"2023-02-04T10:22:09.850881Z","iopub.status.idle":"2023-02-04T10:22:09.857340Z","shell.execute_reply.started":"2023-02-04T10:22:09.850852Z","shell.execute_reply":"2023-02-04T10:22:09.856240Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nimport torch.nn as nn\nimport random\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2023-02-04T10:22:09.862468Z","iopub.execute_input":"2023-02-04T10:22:09.862767Z","iopub.status.idle":"2023-02-04T10:22:09.870078Z","shell.execute_reply.started":"2023-02-04T10:22:09.862742Z","shell.execute_reply":"2023-02-04T10:22:09.868343Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Define transform function\ntrain_tf = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307),(0.3081))\n])\n\ntest_tf = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307),(0.3081))\n])\n","metadata":{"execution":{"iopub.status.busy":"2023-02-04T10:22:09.873488Z","iopub.execute_input":"2023-02-04T10:22:09.873752Z","iopub.status.idle":"2023-02-04T10:22:09.884921Z","shell.execute_reply.started":"2023-02-04T10:22:09.873728Z","shell.execute_reply":"2023-02-04T10:22:09.883876Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Define model\nclass My_model(nn.Module):\n    def __init__(self):\n        super(My_model, self).__init__()\n        \n        self.nn_series = nn.Sequential(                   #output (C, H, W)\n            nn.Conv2d(1, 32, 3, stride=1, padding=1),     #output (32, 28, 28)\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),                        #output (32, 14, 14)\n            \n            nn.Conv2d(32, 64, 3, stride=1, padding=1),    #output (64, 14, 14)\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),                        #output (64, 7, 7)\n        )\n        \n        self.fn_series = nn.Sequential(\n            nn.Linear(64*7*7, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n    \n    def forward(self, data):\n        out = self.nn_series(data)\n        out = out.view(out.size()[0],-1)\n        \n        return self.fn_series(out)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-04T10:22:09.886829Z","iopub.execute_input":"2023-02-04T10:22:09.887464Z","iopub.status.idle":"2023-02-04T10:22:09.896878Z","shell.execute_reply.started":"2023-02-04T10:22:09.887414Z","shell.execute_reply":"2023-02-04T10:22:09.895969Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Hyper parameters\nseed = 111\nlearning_rate = 1e-3\nbatch_size = 64\nepochs = 20\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\npath = \"./mnist\"\ndata_split = 0.3\n\n# Fix random seed and determinitic\nrandom.seed(seed)\nnp.random.seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2023-02-04T10:22:09.898916Z","iopub.execute_input":"2023-02-04T10:22:09.899550Z","iopub.status.idle":"2023-02-04T10:22:09.907742Z","shell.execute_reply.started":"2023-02-04T10:22:09.899516Z","shell.execute_reply":"2023-02-04T10:22:09.906727Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Set loss function and optimizer.\nmodel = My_model().to(device)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=1e-5)","metadata":{"execution":{"iopub.status.busy":"2023-02-04T10:22:09.909381Z","iopub.execute_input":"2023-02-04T10:22:09.909799Z","iopub.status.idle":"2023-02-04T10:22:09.934684Z","shell.execute_reply.started":"2023-02-04T10:22:09.909751Z","shell.execute_reply":"2023-02-04T10:22:09.933694Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Download MNIST data and split the data into training and validation sets.\ntrain_data = datasets.MNIST(root=path, train=True, transform=train_tf, download=True)\nvalid_length = int(len(train_data)*data_split)\ntrain_length = len(train_data)-valid_length\ntrain_set, valid_set = random_split(train_data, [train_length, valid_length], torch.Generator().manual_seed(seed))\n\ntrain_loader = DataLoader(train_set, batch_size, shuffle=True, pin_memory=True)\nvalid_loader = DataLoader(valid_set, batch_size, shuffle=True, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-04T10:22:09.936596Z","iopub.execute_input":"2023-02-04T10:22:09.937213Z","iopub.status.idle":"2023-02-04T10:22:10.027291Z","shell.execute_reply.started":"2023-02-04T10:22:09.937180Z","shell.execute_reply":"2023-02-04T10:22:10.026216Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Training process\nbest_acc = 0\nbest_loss = torch.inf\n\nfor epoch in range(epochs):\n    \n    # Training stage\n    model.train()\n    total_loss = 0\n    acc = []\n    for image, target in train_loader:\n        optimizer.zero_grad()\n        \n        predict = model(image.to(device))\n        loss = loss_fn(predict, target.to(device))\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        acc.append(( predict.argmax(-1) == target.to(device) ).float().mean())\n        \n    mean_train_acc = sum(acc)/len(acc)\n    mean_train_loss = total_loss/len(train_loader)\n    \n    # Valid stage\n    model.eval()\n    total_loss = 0\n    acc = []\n    with torch.no_grad():\n        for image, target in valid_loader:\n            predict = model(image.to(device))\n            loss = loss_fn(predict, target.to(device))\n            \n            total_loss += loss.item()\n            acc.append(( predict.argmax(-1) == target.to(device) ).float().mean())\n        \n        mean_valid_acc = sum(acc)/len(acc)\n        mean_valid_loss = total_loss/len(valid_loader)\n    \n    if best_acc < mean_valid_acc:\n        best_acc = mean_valid_acc\n        torch.save(model.state_dict(), path + \"/model.ckpt\")\n        print(f'epoch: {epoch+1}/{epochs} => train loss: {mean_train_loss}, train acc: {mean_train_acc}/ valid loss: {mean_valid_loss}, valid acc: {mean_valid_acc} => Best accuracy !!')\n    else:\n        print(f'epoch: {epoch+1}/{epochs} => train loss: {mean_train_loss}, train acc: {mean_train_acc}/ valid loss: {mean_valid_loss}, valid acc: {mean_valid_acc}')\n    ","metadata":{"execution":{"iopub.status.busy":"2023-02-04T10:22:10.029822Z","iopub.execute_input":"2023-02-04T10:22:10.030235Z","iopub.status.idle":"2023-02-04T10:26:37.961939Z","shell.execute_reply.started":"2023-02-04T10:22:10.030193Z","shell.execute_reply":"2023-02-04T10:26:37.960830Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"epoch: 1/20 => train loss: 0.13453978600151995, train acc: 0.9574771523475647/ valid loss: 0.05285190119996753, valid acc: 0.983543872833252 => Best accuracy !!\nepoch: 2/20 => train loss: 0.0427317079573925, train acc: 0.986182451248169/ valid loss: 0.06884920044103637, valid acc: 0.9793328642845154\nepoch: 3/20 => train loss: 0.03000703252191213, train acc: 0.9904157519340515/ valid loss: 0.04651563716676674, valid acc: 0.9871453642845154 => Best accuracy !!\nepoch: 4/20 => train loss: 0.020170041908725494, train acc: 0.993008017539978/ valid loss: 0.05507111665410905, valid acc: 0.9852614998817444\nepoch: 5/20 => train loss: 0.01748364122712683, train acc: 0.9941495656967163/ valid loss: 0.04692623305277105, valid acc: 0.987311601638794 => Best accuracy !!\nepoch: 6/20 => train loss: 0.011680142875066576, train acc: 0.9961234927177429/ valid loss: 0.0525284823672098, valid acc: 0.9874777793884277 => Best accuracy !!\nepoch: 7/20 => train loss: 0.011971633606933032, train acc: 0.9961710572242737/ valid loss: 0.05658182253048042, valid acc: 0.9860371947288513\nepoch: 8/20 => train loss: 0.0112309974114205, train acc: 0.9964088797569275/ valid loss: 0.03978612766869891, valid acc: 0.9905252456665039 => Best accuracy !!\nepoch: 9/20 => train loss: 0.008379753179518933, train acc: 0.9972174763679504/ valid loss: 0.05153742322099079, valid acc: 0.9884197115898132\nepoch: 10/20 => train loss: 0.007345568556417295, train acc: 0.9975979924201965/ valid loss: 0.054688649414363175, valid acc: 0.9883089065551758\nepoch: 11/20 => train loss: 0.007566095550212822, train acc: 0.9973839521408081/ valid loss: 0.04878431995966466, valid acc: 0.9891954660415649\nepoch: 12/20 => train loss: 0.007312742669647202, train acc: 0.9975504279136658/ valid loss: 0.047910751837551786, valid acc: 0.9896941184997559\nepoch: 13/20 => train loss: 0.004683668253236349, train acc: 0.9986206293106079/ valid loss: 0.0712075206044948, valid acc: 0.9829897880554199\nepoch: 14/20 => train loss: 0.005946367847345406, train acc: 0.9979785084724426/ valid loss: 0.05831535417851716, valid acc: 0.9889184236526489\nepoch: 15/20 => train loss: 0.00831017318845759, train acc: 0.9973363876342773/ valid loss: 0.047329289773759324, valid acc: 0.9899711608886719\nepoch: 16/20 => train loss: 0.004554129522382712, train acc: 0.9985255002975464/ valid loss: 0.05258098779756086, valid acc: 0.9898603558540344\nepoch: 17/20 => train loss: 0.0030667284327430444, train acc: 0.998810887336731/ valid loss: 0.06674301941238939, valid acc: 0.9882535338401794\nepoch: 18/20 => train loss: 0.005673815790735995, train acc: 0.9981212019920349/ valid loss: 0.04918222304758167, valid acc: 0.9903035759925842\nepoch: 19/20 => train loss: 0.004793078307874085, train acc: 0.9985968470573425/ valid loss: 0.06283745138239685, valid acc: 0.9885305762290955\nepoch: 20/20 => train loss: 0.00462724354061496, train acc: 0.9982876777648926/ valid loss: 0.05084252534974288, valid acc: 0.9903590083122253\n","output_type":"stream"}]},{"cell_type":"code","source":"# Testing process\ntest_data = datasets.MNIST(root=\"./mnist\", train=False, transform=test_tf, download=True)\ntest_loader = DataLoader(test_data, batch_size=64, pin_memory=True)\n\n\ntotal_loss = 0\nacc = []\noutput_list = []\ntarget_list = []\nmodel.eval()\n\nwith torch.no_grad():\n    for image, target in test_loader:\n        predict = model(image.to(device))\n        output_list += predict.argmax(-1).tolist()\n        target_list += target.tolist()\n\n        loss = loss_fn(predict, target.to(device))\n        \n        total_loss += loss.item()\n        acc.append(( predict.argmax(-1) == target.to(device) ).float().mean())\n\nmean_test_acc = sum(acc)/len(acc)\nmean_test_loss = total_loss/len(valid_loader)\n\nprint(f'test loss: {mean_test_loss}, test acc: {mean_test_acc}')\n\n# Save predict value\ndf = pd.DataFrame()\ndf[\"ID\"] = [ i for i in range(1, len(output_list)+1)]\ndf[\"Predict Number\"] = output_list\ndf[\"Target Number\"] = target_list\n\ndf.to_csv(\"./predict.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-02-04T10:26:37.963522Z","iopub.execute_input":"2023-02-04T10:26:37.963960Z","iopub.status.idle":"2023-02-04T10:26:39.811406Z","shell.execute_reply.started":"2023-02-04T10:26:37.963924Z","shell.execute_reply":"2023-02-04T10:26:39.810436Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"test loss: 0.023500364182168717, test acc: 0.990744411945343\n","output_type":"stream"}]}]}